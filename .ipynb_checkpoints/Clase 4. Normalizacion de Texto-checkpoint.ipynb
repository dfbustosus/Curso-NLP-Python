{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65fb82c",
   "metadata": {},
   "source": [
    "# Normalizacion de texto\n",
    "\n",
    "La normalización de texto se define como un proceso que consiste en una serie de pasos que se deben seguir para limpiar y estandarizar los datos textuales en una forma que puedan ser consumidos por otros sistemas y aplicaciones de NLP como entrada.\n",
    "\n",
    "A menudo, la tokenización en sí también es parte de la normalización del texto. Además de la tokenización, varias otras técnicas incluyen la limpieza del texto, la conversión de mayúsculas y minúsculas, la corrección ortográfica, la eliminación de palabras vacías y otros términos innecesarios, la derivación y la lematización. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef66877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "corpus = [\"The brown fox wasn't that quick and he couldn't win the race\",\n",
    "          \"Hey that's a great deal! I just bought a phone for $199\",\n",
    "          \"@@You'll (learn) a **lot** in the book. Python is an amazing language !@@\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb28dcbc",
   "metadata": {},
   "source": [
    "A menudo, los datos textuales que queremos usar o analizar contienen muchos tokens y caracteres extraños e innecesarios que deben eliminarse antes de realizar otras operaciones como tokenización u otras técnicas de normalización. Esto incluye extraer texto significativo de fuentes de datos como datos HTML o incluso datos de fuentes XML y JSON.\n",
    "\n",
    "Hay muchas formas de analizar y limpiar estos datos para eliminar etiquetas innecesarias. Puedes usar funciones como clean_html() de nltk o incluso la biblioteca BeautifulSoup para analizar datos HTML. También puedes usar su propia lógica personalizada, incluidas expresiones regulares, xpath y la biblioteca lxml, para analizar datos XML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be53735",
   "metadata": {},
   "source": [
    "## 1. Tokenizacion\n",
    "Por lo general, tokenizamos el texto antes o después de eliminar caracteres y símbolos innecesarios de los datos. Esta elección depende del problema que está tratando de resolver y de los datos con los que está tratando. Ya hemos visto varias técnicas de tokenización en el video anterior. Definiremos una función de tokenización genérica aquí y ejecutaremos lo mismo en nuestro corpus mencionado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1296eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1936d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']], [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'], ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']], [['@', '@', 'You', \"'ll\", '(', 'learn', ')', 'a', '*', '*', 'lot', '*', '*', 'in', 'the', 'book', '.'], ['Python', 'is', 'an', 'amazing', 'language', '!'], ['@', '@']]]\n"
     ]
    }
   ],
   "source": [
    "token_list = [tokenize_text(text) for text in corpus]\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c7b5d",
   "metadata": {},
   "source": [
    "## 2. Remocion de carcateres especiales\n",
    "\n",
    "Una tarea importante en la normalización de texto consiste en eliminar caracteres innecesarios y especiales. Estos pueden ser símbolos especiales o incluso signos de puntuación que aparecen en las oraciones.\n",
    "\n",
    "Este paso a menudo se realiza antes o después de la tokenización. La razón principal para hacerlo es que, a menudo, la puntuación o los caracteres especiales no tienen mucha importancia cuando analizamos el texto y lo utilizamos para extraer características o información basada en NLP y ML.\n",
    "\n",
    "Implementaremos ambos tipos de eliminación de caracteres especiales, antes y después de la tokenización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23961540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f0ae60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<filter object at 0x000002281AAA03A0>, <filter object at 0x000002281AAA0AC0>, <filter object at 0x000002281AAA0640>]\n"
     ]
    }
   ],
   "source": [
    "# Funcion para limpieza antes de tokenizacion\n",
    "filtered_list_1 = [filter(None,[remove_characters_after_tokenization(tokens) \\\n",
    "                                for tokens in sentence_tokens]) \\\n",
    "                   for sentence_tokens in token_list]\n",
    "print(filtered_list_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d0e331",
   "metadata": {},
   "source": [
    "Esencialmente, lo que hacemos aquí es usar el atributo `string.punctuation`, que consta de todos los caracteres/símbolos especiales posibles, y crear un patrón de expresiones regulares a partir de él. Lo usamos para unir tokens que son símbolos y caracteres y eliminarlos. La función de filter nos ayuda a eliminar los tokens vacíos obtenidos después de eliminar los tokens de caracteres especiales usando el submétodo regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cee4c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters_before_tokenization(sentence, keep_apostrophes=False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    else:\n",
    "        PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f079dd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The brown fox wasnt that quick and he couldnt win the race', 'Hey thats a great deal I just bought a phone for 199', 'Youll learn a lot in the book Python is an amazing language ']\n"
     ]
    }
   ],
   "source": [
    "filtered_list_2 = [remove_characters_before_tokenization(sentence) for sentence in corpus]\n",
    "print(filtered_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e668607d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The brown fox wasn't that quick and he couldn't win the race\",\n",
       " \"Hey that's a great deal! I just bought a phone for 199\",\n",
       " \"You'll learn a lot in the book. Python is an amazing language !\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = [remove_characters_before_tokenization(sentence,keep_apostrophes=True)\\\n",
    "                  for sentence in corpus]\n",
    "cleaned_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d5639",
   "metadata": {},
   "source": [
    "Los resultados anteriores muestran dos formas diferentes de eliminar caracteres especiales antes de la tokenización: eliminar todos los caracteres especiales en lugar de conservar los apóstrofes utilizando expresiones regulares. Por lo general, después de eliminar estos caracteres, puede tomar el texto limpio y tokenizarlo o aplicarle otras operaciones de normalización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c95ff08",
   "metadata": {},
   "source": [
    "## 3. Expansion de Contracciones\n",
    "\n",
    "Las contracciones son versiones abreviadas de palabras o sílabas. Existen en formas escritas o habladas. Se crean versiones abreviadas de palabras existentes eliminando letras y sonidos específicos. En el caso de las contracciones en inglés, a menudo se crean eliminando una de las vocales de la palabra. Los ejemplos serían is not to is not y will not to will not , donde puede notar que el apóstrofe se usa para denotar la contracción y algunas de las vocales y otras letras se eliminan. Por lo general, las contracciones se evitan cuando se usan en la escritura formal, pero de manera informal, se usan bastante. Existen varias formas de contracciones que están ligadas al tipo de verbos auxiliares que nos dan contracciones normales, contracciones negadas y otras contracciones coloquiales especiales, algunas de las cuales pueden no tener auxiliares.\n",
    " \n",
    "Recuerde, sin embargo, que algunas de las contracciones pueden tener múltiples formas. Para simplificar, he tomado una de estas formas expandidas para cada contracción. El siguiente paso, para expandir las contracciones, usa el siguiente fragmento de código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4b6f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25ba5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(sentence, contraction_mapping):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "    return expanded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b1c73d",
   "metadata": {},
   "source": [
    "El codigo anterior usa función `expand_match` dentro de la función principal `expand_contractions` para encontrar cada contracción que coincida con el patrón de expresiones regulares que creamos a partir de todas las contracciones en nuestro diccionario `CONTRACTION_MAP`. Al emparejar cualquier contracción, la sustituimos con su correspondiente versión expandida y conservamos el caso correcto de la palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "800f2529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The brown fox was not that quick and he could not win the race', 'Hey that is a great deal! I just bought a phone for 199', 'You will learn a lot in the book. Python is an amazing language !']\n"
     ]
    }
   ],
   "source": [
    "expanded_corpus = [expand_contractions(sentence, CONTRACTION_MAP) for sentence in cleaned_corpus]\n",
    "print(expanded_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d100ec",
   "metadata": {},
   "source": [
    "## 4. Case Conversion\n",
    "\n",
    "A menudo queremos modificar las mayúsculas y minúsculas de las palabras o las oraciones para facilitar las cosas, como hacer coincidir palabras o tokens específicos. Por lo general, hay dos tipos de operaciones de conversión de casos que se usan mucho. Estas son conversiones de minúsculas y mayúsculas, donde un cuerpo de texto se convierte completamente a minúsculas o mayúsculas. También hay otras formas, como el caso de la oración o el caso propio. Minúsculas es una forma donde todas las letras del texto son minúsculas, y en mayúsculas todas están en mayúscula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0a69109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the brown fox wasn't that quick and he couldn't win the race\n",
      "THE BROWN FOX WASN'T THAT QUICK AND HE COULDN'T WIN THE RACE\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0].lower())\n",
    "print(corpus[0].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59dca6c",
   "metadata": {},
   "source": [
    "## 5. Remover Stopwords\n",
    "\n",
    "Las palabras vacías, a veces palabras vacías escritas, son palabras que tienen poco o ningún significado. Por lo general, se eliminan del texto durante el procesamiento para conservar las palabras que tienen el máximo significado y contexto. Las palabras vacías suelen ser palabras que terminan apareciendo con mayor frecuencia si agrega cualquier corpus de texto basado en tokens singulares y verifica sus frecuencias. Palabras como a, the , me , etc. son palabras vacías. No existe una lista universal o exhaustiva de palabras vacías. Cada dominio o idioma puede tener su propio conjunto de palabras vacías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7a3c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65cbb01",
   "metadata": {},
   "source": [
    "Aprovechamos el uso de nltk , que tiene una lista de palabras vacías para inglés filtramos todos los tokens que correspondan a palabras vacías. Usamos nuestra función `tokenize_text` para tokenizar el cuerpo expandido que obtuvimos en la sección anterior y luego eliminamos las palabras vacías necesarias usando la función anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bc2f4e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['The',\n",
       "   'brown',\n",
       "   'fox',\n",
       "   'was',\n",
       "   'not',\n",
       "   'that',\n",
       "   'quick',\n",
       "   'and',\n",
       "   'he',\n",
       "   'could',\n",
       "   'not',\n",
       "   'win',\n",
       "   'the',\n",
       "   'race']],\n",
       " [['Hey', 'that', 'is', 'a', 'great', 'deal', '!'],\n",
       "  ['I', 'just', 'bought', 'a', 'phone', 'for', '199']],\n",
       " [['You', 'will', 'learn', 'a', 'lot', 'in', 'the', 'book', '.'],\n",
       "  ['Python', 'is', 'an', 'amazing', 'language', '!']]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_corpus_tokens = [tokenize_text(text) for text in expanded_corpus]\n",
    "expanded_corpus_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fea3911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['The', 'brown', 'fox', 'quick', 'could', 'win', 'race']],\n",
       " [['Hey', 'great', 'deal', '!'], ['I', 'bought', 'phone', '199']],\n",
       " [['You', 'learn', 'lot', 'book', '.'],\n",
       "  ['Python', 'amazing', 'language', '!']]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_list_3 = [[remove_stopwords(tokens) for tokens in sentence_tokens] \\\n",
    "                   for sentence_tokens in expanded_corpus_tokens]\n",
    "filtered_list_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e84d3",
   "metadata": {},
   "source": [
    "## 6. Correccion de palabras\n",
    "\n",
    "Texto incorrecto puede ser palabras que tienen errores de ortografía, así como palabras con varias letras repetidas que no contribuyen mucho a su significado general. Para ilustrar algunos ejemplos, la palabra **finally^^ podría escribirse erróneamente como **fianlly**, o alguien que exprese una emoción intensa podría escribirla como **finalllllyyyyyy**.\n",
    "\n",
    "### 6.1 Correccion de caracteres repetidos\n",
    "El primer paso en nuestro algoritmo sería identificar los caracteres repetidos en una palabra usando un patrón de expresiones regulares y luego usar una sustitución para eliminar los caracteres uno por uno.\n",
    "\n",
    "Considera la palabra **finallyyy** del ejemplo anterior. El patrón `r'(\\w*)(\\w)\\2(\\w*)'` puede usarse para identificar caracteres que ocurren dos veces entre otros caracteres de la palabra, y en cada paso intentaremos eliminar uno de los repetidos usando una sustitución para la coincidencia utilizando los grupos de coincidencia de expresiones regulares (grupos 1, 2 y 3) usando el patrón `r'\\1\\2\\3'` y luego se sigue iterando a través de este proceso hasta que no queden caracteres repetidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd0a4bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso: 1 Palabra: finalllyy\n",
      "Paso: 2 Palabra: finallly\n",
      "Paso: 3 Palabra: finally\n",
      "Paso: 4 Palabra: finaly\n",
      "Final word: finaly\n"
     ]
    }
   ],
   "source": [
    "old_word = 'finalllyyy'\n",
    "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "match_substitution = r'\\1\\2\\3'\n",
    "step = 1\n",
    "while True:\n",
    "    # remove one repeated character\n",
    "    new_word = repeat_pattern.sub(match_substitution,old_word)\n",
    "    if new_word != old_word:\n",
    "        print('Paso: {} Palabra: {}'.format(step, new_word))\n",
    "        step += 1 # update step \n",
    "        # update old word to last substituted state\n",
    "        old_word = new_word\n",
    "        continue\n",
    "    else:\n",
    "        print(\"Final word:\", new_word)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b3a701",
   "metadata": {},
   "source": [
    "El fragmento anterior muestra cómo se elimina un carácter repetido en cada etapa hasta que terminamos con la palabra finalmente al *finaly*. Sin embargo, semánticamente, esta palabra es incorrecta: la palabra correcta fue **finally**, que obtuvimos en el paso 3. Ahora utilizaremos el corpus de WordNet para verificar las palabras válidas en cada etapa y terminar el ciclo una vez se obtiene la opcion correcta. Esto introduce la corrección semántica necesaria para nuestro algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a449557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso: 1 Palabras: finalllyy\n",
      "Paso: 2 Palabras: finallly\n",
      "Paso: 3 Palabras: finally\n",
      "Palabra final corregida: finally\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "old_word = 'finalllyyy'\n",
    "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "match_substitution = r'\\1\\2\\3'\n",
    "step = 1\n",
    "while True:\n",
    "    # chequear la semantica \n",
    "    if wordnet.synsets(old_word):\n",
    "        print(\"Palabra final corregida:\", old_word)\n",
    "        break\n",
    "    # remover  caracteres repetidos\n",
    "    new_word = repeat_pattern.sub(match_substitution,old_word)\n",
    "    if new_word != old_word:\n",
    "        print('Paso: {} Palabras: {}'.format(step, new_word))\n",
    "        step += 1 # update \n",
    "        # update la old word por su estado cambiado\n",
    "        old_word = new_word\n",
    "        continue\n",
    "    else:\n",
    "        print(\"Palabra final:\", new_word)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8673fefd",
   "metadata": {},
   "source": [
    "Ahora si lo hacemos mas generico tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6ac6d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac15c055",
   "metadata": {},
   "source": [
    "Ese fragmento usa la función interna `replace()` para emular básicamente el comportamiento de nuestro algoritmo, ilustrado anteriormente, y luego llamarlo repetidamente en cada token en una oración en la función externa `remove_repeated_characters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6612d0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'schooool', 'is', 'realllllyyy', 'amaaazingggg']\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = 'My schooool is realllllyyy amaaazingggg'\n",
    "sample_sentence_tokens = tokenize_text(sample_sentence)[0]\n",
    "print(sample_sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "750baf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'school', 'is', 'really', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "print(remove_repeated_characters(sample_sentence_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42526069",
   "metadata": {},
   "source": [
    "PErfecto hasta acá hemos realizado varias tareas como **Tokenizar, Remover Caracteres Especiales, Expandir Contracciones, Convertir a minusculas, remocion de stopwords así como remover caracteres repetidos en palabras**. En el proximo tutorial veremos como terminar de hacer la correccion ortografica de nuestro texto y aprenderas sobre los procesos de Stemming y Lemantizacion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
